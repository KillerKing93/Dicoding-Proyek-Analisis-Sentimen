{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Proyek Analisis Sentimen Ulasan Google Play Store (Target Bintang 5)\n",
        "\n",
        "Notebook ini berisi langkah-langkah untuk melakukan analisis sentimen pada ulasan aplikasi dari Google Play Store, dirancang untuk memenuhi semua kriteria dan saran untuk mendapatkan penilaian bintang 5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SEL 1: INSTALASI & IMPORT LIBRARY\n",
        "\n",
        "Bagian ini mengimpor semua library yang diperlukan. Pastikan semua library ini tercantum dalam file `requirements.txt` Anda."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Pastikan library ini ada di requirements.txt\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# !pip install pandas numpy nltk scikit-learn tensorflow matplotlib seaborn sastrawi # Jika menggunakan Sastrawi\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m \u001b[38;5;66;03m# Modul regular expression untuk cleaning\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
          ]
        }
      ],
      "source": [
        "# Pastikan library ini ada di requirements.txt\n",
        "# !pip install pandas numpy nltk scikit-learn tensorflow matplotlib seaborn sastrawi # Jika menggunakan Sastrawi\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re # Modul regular expression untuk cleaning\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download resource NLTK (hanya perlu sekali, uncomment jika belum)\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except nltk.downloader.DownloadError:\n",
        "    nltk.download('punkt')\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except nltk.downloader.DownloadError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "# Scikit-learn untuk preprocessing, model ML, evaluasi\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# TensorFlow / Keras untuk Deep Learning\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout, GlobalMaxPool1D\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Visualisasi\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set opsi Pandas agar teks terlihat penuh\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# (Opsional) Sastrawi untuk stemming Bahasa Indonesia\n",
        "# from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "# try:\n",
        "#    factory = StemmerFactory()\n",
        "#    stemmer = factory.create_stemmer()\n",
        "# except ImportError:\n",
        "#    print(\"Sastrawi tidak terinstall. Stemming tidak akan dilakukan.\")\n",
        "#    stemmer = None # Tandai bahwa stemmer tidak tersedia\n",
        "\n",
        "print(\"Library berhasil diimport.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SEL 2: MEMUAT DATASET\n",
        "\n",
        "Memuat dataset CSV yang dihasilkan dari skrip scraping. Pastikan nama file sesuai."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NAMA_FILE_DATASET = 'hasil_scraping_semua_app.csv' # <<< NAMA FILE DATA YANG AKAN DIMUAT\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(NAMA_FILE_DATASET)\n",
        "    print(f\"Dataset '{NAMA_FILE_DATASET}' berhasil dimuat.\")\n",
        "    print(f\"Jumlah data awal: {len(df)}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: File '{NAMA_FILE_DATASET}' tidak ditemukan. Pastikan file ada di direktori yang sama atau path sudah benar.\")\n",
        "    # Hentikan eksekusi jika file tidak ada (dalam notebook mungkin lebih baik raise error)\n",
        "    raise FileNotFoundError(f\"Dataset {NAMA_FILE_DATASET} tidak ditemukan.\")\n",
        "\n",
        "# Tampilkan beberapa data awal dan info\n",
        "print(\"\\nInfo Dataset:\")\n",
        "df.info()\n",
        "print(\"\\n5 Data Pertama:\")\n",
        "print(df.head())\n",
        "print(\"\\nContoh Ulasan:\")\n",
        "# Handle jika dataset kosong atau tidak punya baris ke-0\n",
        "if not df.empty:\n",
        "    print(df['content'].iloc[0])\n",
        "else:\n",
        "    print(\"Dataset kosong.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SEL 3: DATA CLEANING & PREPROCESSING\n",
        "\n",
        "Tahapan ini meliputi:\n",
        "1.  Handling Missing Values.\n",
        "2.  Membuat Label Sentimen (3 Kelas: Positif, Negatif, Netral) -> **Memenuhi Syarat 3 Kelas**.\n",
        "3.  Text Cleaning (lowercase, hapus URL, tanda baca, angka, stopwords Bhs. Indonesia)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nMemulai Data Cleaning & Preprocessing...\")\n",
        "\n",
        "# 1. Handling Missing Values (jika ada di kolom 'content' atau 'score')\n",
        "print(f\"\\nJumlah missing values sebelum handling:\\n{df.isnull().sum()}\")\n",
        "df.dropna(subset=['content', 'score'], inplace=True)\n",
        "# Konversi kolom 'score' ke tipe numerik yg sesuai (misal, int jika perlu)\n",
        "df['score'] = pd.to_numeric(df['score'], errors='coerce') # Ubah ke float, paksa non-numerik jadi NaN\n",
        "df.dropna(subset=['score'], inplace=True) # Hapus baris yg score-nya NaN setelah konversi\n",
        "df['score'] = df['score'].astype(int) # Ubah ke integer\n",
        "\n",
        "# Reset index setelah dropna\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "print(f\"\\nJumlah missing values setelah handling:\\n{df.isnull().sum()}\")\n",
        "print(f\"Jumlah data setelah handling missing values: {len(df)}\")\n",
        "\n",
        "# 2. Membuat Label Sentimen (3 Kelas: Positif, Negatif, Netral)\n",
        "def create_sentiment_label(score):\n",
        "    if score > 3:\n",
        "        return 'Positif' # Rating 4-5\n",
        "    elif score < 3:\n",
        "        return 'Negatif' # Rating 1-2\n",
        "    else:\n",
        "        return 'Netral'  # Rating 3\n",
        "\n",
        "df['sentiment'] = df['score'].apply(create_sentiment_label)\n",
        "print(\"\\nDistribusi Sentimen:\")\n",
        "print(df['sentiment'].value_counts())\n",
        "\n",
        "# Tampilkan data dengan label sentimen\n",
        "print(\"\\nData dengan Label Sentimen:\")\n",
        "print(df[['score', 'content', 'sentiment']].head())\n",
        "\n",
        "# 3. Text Cleaning Function\n",
        "# Daftar stopwords Bahasa Indonesia (menggunakan NLTK)\n",
        "try:\n",
        "    list_stopwords = stopwords.words('indonesian')\n",
        "except LookupError:\n",
        "    print(\"Stopwords NLTK untuk Bahasa Indonesia belum diunduh. Menjalankan nltk.download('stopwords')...\")\n",
        "    nltk.download('stopwords')\n",
        "    list_stopwords = stopwords.words('indonesian')\n",
        "\n",
        "# Tambahkan kata umum non-informatif lainnya jika perlu\n",
        "list_stopwords.extend([\"yg\", \"dg\", \"rt\", \"dgn\", \"ny\", \"d\", 'klo',\n",
        "                       'kalo', 'amp', 'biar', 'bikin', 'bilang',\n",
        "                       'gak', 'ga', 'krn', 'nya', 'nih', 'sih',\n",
        "                       'si', 'tau', 'tdk', 'tuh', 'utk', 'ya',\n",
        "                       'jd', 'jgn', 'sdh', 'aja', 'n', 't',\n",
        "                       'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt',\n",
        "                       '&amp', 'yah', 'dst', 'dll', 'dah', 'deh', 'tokped', # Tokped karena data dari sana\n",
        "                       'aplikasi', 'app', 'sangat', 'sekali', 'mantap', 'keren', # Kata umum lainnya\n",
        "                      ])\n",
        "\n",
        "# Hapus stopwords yang mungkin relevan untuk sentimen (jika diperlukan)\n",
        "# Contoh: hapus 'tidak' dari stopwords agar negasi tetap ada\n",
        "# st_words_to_keep = {'tidak', 'kurang', 'belum', 'jangan'}\n",
        "# list_stopwords = set(list_stopwords) - st_words_to_keep\n",
        "list_stopwords = set(list_stopwords)\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    # Pastikan input adalah string\n",
        "    if not isinstance(text, str):\n",
        "        return \"\" # Kembalikan string kosong jika bukan string\n",
        "    # Hapus @username\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    # Hapus URL\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    # Hapus karakter HTML (jika ada)\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    # Hapus tanda baca dan angka\n",
        "    # String punctuation: '!”#$%&'()*+,-./:;<=>?@[]^_`{|}~'\n",
        "    # Lebih baik hapus tanda baca spesifik yg tidak penting, atau biarkan jika relevan (misal !?)\n",
        "    # text = text.translate(str.maketrans('', '', string.punctuation + string.digits))\n",
        "    # Versi lebih hati-hati: hapus angka, dan beberapa tanda baca umum\n",
        "    text = re.sub(r'\\d+', '', text) # Hapus angka\n",
        "    text = text.translate(str.maketrans('', '', '\"#$%&()*+,-./:;<=>@[]^_`{|}~')) # Hapus tanda baca tertentu\n",
        "    # Ubah ke lowercase\n",
        "    text = text.lower()\n",
        "    # Hapus whitespace berlebih\n",
        "    text = text.strip()\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # Tokenisasi\n",
        "    try:\n",
        "        tokens = word_tokenize(text)\n",
        "    except LookupError:\n",
        "        print(\"Tokenizer NLTK (punkt) belum diunduh. Menjalankan nltk.download('punkt')...\")\n",
        "        nltk.download('punkt')\n",
        "        tokens = word_tokenize(text)\n",
        "    # Hapus stopwords\n",
        "    tokens = [word for word in tokens if word not in list_stopwords and len(word) > 1] # Abaikan token 1 huruf\n",
        "    # Gabungkan kembali token menjadi string\n",
        "    cleaned_text = ' '.join(tokens)\n",
        "\n",
        "    # (Opsional) Stemming dengan Sastrawi (bisa memakan waktu)\n",
        "    # if stemmer:\n",
        "    #    cleaned_text = stemmer.stem(cleaned_text)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "# Terapkan fungsi cleaning ke kolom 'content'\n",
        "print(\"\\nMenerapkan cleaning ke kolom 'content'... (mungkin perlu waktu)\")\n",
        "# Gunakan salinan untuk menghindari SettingWithCopyWarning\n",
        "df_cleaned = df.copy()\n",
        "df_cleaned['cleaned_content'] = df_cleaned['content'].apply(clean_text)\n",
        "\n",
        "# Tampilkan hasil cleaning pada beberapa data\n",
        "print(\"\\nContoh Hasil Cleaning:\")\n",
        "print(df_cleaned[['content', 'cleaned_content', 'sentiment']].head())\n",
        "\n",
        "# Cek jika ada hasil cleaning yang kosong (mungkin karena hanya berisi stopwords/angka/tanda baca)\n",
        "empty_cleaned_indices = df_cleaned[df_cleaned['cleaned_content'] == ''].index\n",
        "print(f\"\\nJumlah ulasan yang kosong setelah cleaning: {len(empty_cleaned_indices)}\")\n",
        "df_cleaned.drop(empty_cleaned_indices, inplace=True)\n",
        "df_cleaned.reset_index(drop=True, inplace=True)\n",
        "print(f\"Jumlah data setelah menghapus hasil cleaning kosong: {len(df_cleaned)}\")\n",
        "\n",
        "# Cek jika data masih cukup (>10000 untuk bintang 5, >3000 minimal)\n",
        "if len(df_cleaned) < 3000:\n",
        "    print(\"PERINGATAN: Jumlah data setelah cleaning kurang dari 3000. Mungkin perlu scraping lebih banyak atau perbaiki proses cleaning.\")\n",
        "elif len(df_cleaned) < 10000:\n",
        "    print(\"INFO: Jumlah data setelah cleaning antara 3000 dan 10000.\")\n",
        "else:\n",
        "    print(\"INFO: Jumlah data setelah cleaning >= 10000. (Memenuhi Saran Jumlah Data)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SEL 4: PEMBAGIAN DATA (TRAIN & TEST)\n",
        "\n",
        "Membagi data menjadi set pelatihan dan pengujian. Dilakukan dua kali dengan rasio berbeda (80/20 dan 70/30) untuk memenuhi syarat 3 skema eksperimen. Menggunakan `stratify` untuk menjaga proporsi kelas sentimen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pisahkan fitur (X) dan label (y)\n",
        "X = df_cleaned['cleaned_content']\n",
        "y = df_cleaned['sentiment']\n",
        "\n",
        "# Encode label menjadi numerik (penting untuk model ML/DL)\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "# Lihat mapping label asli ke numerik\n",
        "print(\"\\nMapping Label ke Numerik:\")\n",
        "label_mapping = {label: i for i, label in enumerate(label_encoder.classes_)}\n",
        "print(label_mapping)\n",
        "num_classes = len(label_encoder.classes_)\n",
        "print(f\"Jumlah kelas: {num_classes}\")\n",
        "\n",
        "# Split data menjadi training (80%) dan testing (20%)\n",
        "# Menggunakan stratify=y_encoded agar proporsi kelas sentimen sama di train dan test set\n",
        "X_train, X_test, y_train_encoded, y_test_encoded = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "# Split data lagi untuk eksperimen dengan split berbeda (misal 70/30)\n",
        "X_train_70, X_test_30, y_train_encoded_70, y_test_encoded_30 = train_test_split(\n",
        "    X, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "\n",
        "print(f\"\\nUkuran data training (80/20): {len(X_train)}, Testing: {len(X_test)}\")\n",
        "print(f\"Distribusi kelas di y_train_encoded (80/20): {np.bincount(y_train_encoded)}\")\n",
        "print(f\"Distribusi kelas di y_test_encoded (80/20): {np.bincount(y_test_encoded)}\")\n",
        "\n",
        "print(f\"\\nUkuran data training (70/30): {len(X_train_70)}, Testing: {len(X_test_30)}\")\n",
        "print(f\"Distribusi kelas di y_train_encoded_70 (70/30): {np.bincount(y_train_encoded_70)}\")\n",
        "print(f\"Distribusi kelas di y_test_encoded_30 (70/30): {np.bincount(y_test_encoded_30)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SEL 5: EKSPERIMEN 1 - SVM + TF-IDF (Split 80/20)\n",
        "\n",
        "Eksperimen pertama menggunakan Support Vector Machine (SVM) dengan fitur TF-IDF dan pembagian data 80/20."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n--- EKSPERIMEN 1: SVM + TF-IDF (Split 80/20) ---\")\n",
        "\n",
        "# 1. Feature Extraction: TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000) # Batasi jumlah fitur (bisa disesuaikan)\n",
        "\n",
        "# Fit dan transform training data\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform testing data (HANYA transform, jangan fit lagi)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "print(f\"Dimensi TF-IDF Training: {X_train_tfidf.shape}\")\n",
        "print(f\"Dimensi TF-IDF Testing: {X_test_tfidf.shape}\")\n",
        "\n",
        "# 2. Model Training: SVM\n",
        "print(\"\\nMelatih model SVM...\")\n",
        "svm_model = SVC(kernel='linear', C=1.0, random_state=42, probability=True) # Linear kernel sering bagus untuk teks, C=regularization\n",
        "svm_model.fit(X_train_tfidf, y_train_encoded)\n",
        "print(\"Pelatihan SVM selesai.\")\n",
        "\n",
        "# 3. Prediksi & Evaluasi\n",
        "y_pred_svm = svm_model.predict(X_test_tfidf)\n",
        "\n",
        "accuracy_svm = accuracy_score(y_test_encoded, y_pred_svm)\n",
        "report_svm = classification_report(y_test_encoded, y_pred_svm, target_names=label_encoder.classes_, zero_division=0)\n",
        "conf_matrix_svm = confusion_matrix(y_test_encoded, y_pred_svm)\n",
        "\n",
        "print(f\"\\nAkurasi SVM (Test Set): {accuracy_svm:.4f}\")\n",
        "# Target >85% terpenuhi?\n",
        "if accuracy_svm >= 0.85:\n",
        "    print(\"✅ Akurasi Test Set SVM >= 85% (Kriteria Utama Terpenuhi)\")\n",
        "else:\n",
        "    print(\"⚠️ Akurasi Test Set SVM < 85% (Perlu Peningkatan)\")\n",
        "\n",
        "print(\"\\nClassification Report SVM:\")\n",
        "print(report_svm)\n",
        "\n",
        "print(\"\\nConfusion Matrix SVM:\")\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(conf_matrix_svm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix - SVM (80/20)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SEL 6: EKSPERIMEN 2 - Random Forest + TF-IDF (Split 70/30)\n",
        "\n",
        "Eksperimen kedua menggunakan Random Forest dengan fitur TF-IDF, namun dengan pembagian data yang berbeda (70/30). Ini memberikan variasi pada **algoritma** dan **pembagian data** dibandingkan Eksperimen 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Variasi: Algoritma (RF) dan Pembagian Data (70/30)\n",
        "print(\"\\n--- EKSPERIMEN 2: Random Forest + TF-IDF (Split 70/30) ---\")\n",
        "\n",
        "# 1. Feature Extraction: TF-IDF (menggunakan data split 70/30)\n",
        "tfidf_vectorizer_70 = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "# Fit dan transform training data (70%)\n",
        "X_train_tfidf_70 = tfidf_vectorizer_70.fit_transform(X_train_70)\n",
        "# Transform testing data (30%)\n",
        "X_test_tfidf_30 = tfidf_vectorizer_70.transform(X_test_30)\n",
        "\n",
        "print(f\"Dimensi TF-IDF Training (70%): {X_train_tfidf_70.shape}\")\n",
        "print(f\"Dimensi TF-IDF Testing (30%): {X_test_tfidf_30.shape}\")\n",
        "\n",
        "# 2. Model Training: Random Forest\n",
        "print(\"\\nMelatih model Random Forest...\")\n",
        "rf_model = RandomForestClassifier(n_estimators=150, max_depth=None, min_samples_split=2, min_samples_leaf=1, random_state=42, n_jobs=-1) # n_jobs=-1 pakai semua core CPU\n",
        "rf_model.fit(X_train_tfidf_70, y_train_encoded_70)\n",
        "print(\"Pelatihan Random Forest selesai.\")\n",
        "\n",
        "# 3. Prediksi & Evaluasi\n",
        "y_pred_rf = rf_model.predict(X_test_tfidf_30)\n",
        "\n",
        "accuracy_rf = accuracy_score(y_test_encoded_30, y_pred_rf)\n",
        "report_rf = classification_report(y_test_encoded_30, y_pred_rf, target_names=label_encoder.classes_, zero_division=0)\n",
        "conf_matrix_rf = confusion_matrix(y_test_encoded_30, y_pred_rf)\n",
        "\n",
        "print(f\"\\nAkurasi Random Forest (Test Set 70/30): {accuracy_rf:.4f}\")\n",
        "# Target >85% terpenuhi?\n",
        "if accuracy_rf >= 0.85:\n",
        "    print(\"✅ Akurasi Test Set RF >= 85% (Kriteria Utama Terpenuhi)\")\n",
        "else:\n",
        "    print(\"⚠️ Akurasi Test Set RF < 85% (Perlu Peningkatan)\")\n",
        "\n",
        "print(\"\\nClassification Report Random Forest:\")\n",
        "print(report_rf)\n",
        "\n",
        "print(\"\\nConfusion Matrix Random Forest:\")\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(conf_matrix_rf, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix - Random Forest (70/30)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SEL 7: EKSPERIMEN 3 - Deep Learning (LSTM) + Sequence Padding (Split 80/20)\n",
        "\n",
        "Eksperimen ketiga menggunakan model Deep Learning (LSTM) dengan metode feature extraction yang berbeda (Sequence Padding menggunakan Keras Tokenizer). Pembagian data kembali ke 80/20. Ini memberikan variasi pada **algoritma (Deep Learning)** dan **metode ekstraksi fitur** dibandingkan Eksperimen 1 & 2. -> **Memenuhi Saran Menggunakan Deep Learning** dan target akurasi >92% akan dicek."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Variasi: Algoritma (LSTM - Deep Learning) dan Feature Extraction (Sequence Padding)\n",
        "print(\"\\n--- EKSPERIMEN 3: Deep Learning (LSTM) + Sequence Padding (Split 80/20) ---\")\n",
        "\n",
        "# 1. Feature Extraction: Tokenizer & Padding\n",
        "MAX_VOCAB_SIZE = 15000 # Ukuran kosakata (bisa disesuaikan)\n",
        "MAX_SEQUENCE_LENGTH = 120 # Panjang maksimum sekuens (bisa disesuaikan)\n",
        "EMBEDDING_DIM = 128 # Dimensi vektor embedding (bisa disesuaikan)\n",
        "\n",
        "# Buat tokenizer Keras\n",
        "keras_tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token='<OOV>') # oov_token untuk kata di luar vocab\n",
        "\n",
        "# Fit tokenizer HANYA pada data training (80/20)\n",
        "keras_tokenizer.fit_on_texts(X_train)\n",
        "word_index = keras_tokenizer.word_index\n",
        "print(f\"Ditemukan {len(word_index)} token unik.\")\n",
        "\n",
        "# Konversi teks ke sekuens integer\n",
        "X_train_seq = keras_tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = keras_tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Padding sekuens agar panjangnya sama\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
        "\n",
        "# One-hot encode label untuk Keras (karena 3 kelas)\n",
        "y_train_keras = to_categorical(y_train_encoded, num_classes=num_classes)\n",
        "y_test_keras = to_categorical(y_test_encoded, num_classes=num_classes)\n",
        "\n",
        "print(f\"\\nDimensi Sekuens Training (Padded): {X_train_pad.shape}\")\n",
        "print(f\"Dimensi Sekuens Testing (Padded): {X_test_pad.shape}\")\n",
        "print(f\"Dimensi Label Training (One-Hot): {y_train_keras.shape}\")\n",
        "print(f\"Dimensi Label Testing (One-Hot): {y_test_keras.shape}\")\n",
        "\n",
        "# 2. Model Building: LSTM\n",
        "print(\"\\nMembangun model LSTM...\")\n",
        "lstm_model = Sequential([\n",
        "    Embedding(input_dim=MAX_VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH),\n",
        "    Bidirectional(LSTM(64, return_sequences=True)), # Bidirectional bisa menangkap konteks dari 2 arah\n",
        "    GlobalMaxPool1D(), # Atau gunakan LSTM(64) saja tanpa return_sequences=True\n",
        "    Dropout(0.3), # Membantu mencegah overfitting\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5), # Dropout lebih besar sebelum output layer\n",
        "    Dense(num_classes, activation='softmax') # Output layer (sesuai jumlah kelas), softmax untuk probabilitas\n",
        "])\n",
        "\n",
        "lstm_model.compile(loss='categorical_crossentropy', # Loss untuk multi-class classification\n",
        "                   optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                   metrics=['accuracy'])\n",
        "\n",
        "lstm_model.summary()\n",
        "\n",
        "# 3. Model Training: LSTM\n",
        "NUM_EPOCHS = 8 # Jumlah epoch (perlu disesuaikan, monitor validation loss)\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)\n",
        "# Simpan model terbaik (opsional)\n",
        "# model_checkpoint = ModelCheckpoint('best_lstm_model.keras', save_best_only=True, monitor='val_accuracy', mode='max', verbose=1)\n",
        "\n",
        "print(\"\\nMemulai pelatihan model LSTM...\")\n",
        "history_lstm = lstm_model.fit(\n",
        "    X_train_pad, y_train_keras,\n",
        "    epochs=NUM_EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_data=(X_test_pad, y_test_keras), # Gunakan test set sebagai validation set di sini\n",
        "    callbacks=[early_stopping], # tambahkan model_checkpoint jika ingin menyimpan model\n",
        "    verbose=1 # Tampilkan progress bar\n",
        ")\n",
        "print(\"Pelatihan LSTM selesai.\")\n",
        "\n",
        "# 4. Evaluasi Model LSTM\n",
        "print(\"\\nMengevaluasi model LSTM pada Test Set (menggunakan bobot terbaik dari EarlyStopping)...\")\n",
        "loss_lstm, accuracy_lstm_test = lstm_model.evaluate(X_test_pad, y_test_keras, verbose=0)\n",
        "\n",
        "# Dapatkan akurasi training dari history (epoch terbaik berdasarkan val_loss jika EarlyStopping aktif)\n",
        "# Jika restore_best_weights=True, model sudah dalam kondisi terbaiknya\n",
        "# Kita bisa evaluasi ulang di training set untuk akurasi train terbaik\n",
        "loss_lstm_train, accuracy_lstm_train = lstm_model.evaluate(X_train_pad, y_train_keras, verbose=0)\n",
        "\n",
        "# Alternatif: ambil dari history jika tidak evaluasi ulang\n",
        "# best_epoch = np.argmin(history_lstm.history['val_loss'])\n",
        "# accuracy_lstm_train = history_lstm.history['accuracy'][best_epoch]\n",
        "# accuracy_lstm_test = history_lstm.history['val_accuracy'][best_epoch]\n",
        "\n",
        "print(f\"\\nAkurasi LSTM (Train Set - Best): {accuracy_lstm_train:.4f}\")\n",
        "print(f\"Akurasi LSTM (Test Set - Best): {accuracy_lstm_test:.4f}\")\n",
        "\n",
        "# Target >92% terpenuhi? (Memenuhi Saran Nilai Tinggi)\n",
        "saran_acc_terpenuhi = accuracy_lstm_train > 0.92 and accuracy_lstm_test > 0.92\n",
        "kriteria_acc_terpenuhi = accuracy_lstm_test >= 0.85\n",
        "\n",
        "if saran_acc_terpenuhi:\n",
        "    print(\"✅ Akurasi Train & Test LSTM > 92% (Saran Nilai Tinggi Terpenuhi)\")\n",
        "elif kriteria_acc_terpenuhi:\n",
        "     print(\"✅ Akurasi Test Set LSTM >= 85% (Kriteria Utama Terpenuhi, Saran >92% belum)\")\n",
        "else:\n",
        "    print(\"⚠️ Akurasi Test Set LSTM < 85% (Perlu Peningkatan)\")\n",
        "\n",
        "\n",
        "# Plotting History Training LSTM\n",
        "if history_lstm:\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Plot Akurasi\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history_lstm.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history_lstm.history['val_accuracy'], label='Validation (Test) Accuracy')\n",
        "    plt.title('Akurasi Model LSTM')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot Loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history_lstm.history['loss'], label='Training Loss')\n",
        "    plt.plot(history_lstm.history['val_loss'], label='Validation (Test) Loss')\n",
        "    plt.title('Loss Model LSTM')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Classification Report & Confusion Matrix untuk LSTM\n",
        "y_pred_lstm_prob = lstm_model.predict(X_test_pad)\n",
        "y_pred_lstm = np.argmax(y_pred_lstm_prob, axis=1) # Ambil kelas dengan probabilitas tertinggi\n",
        "\n",
        "report_lstm = classification_report(y_test_encoded, y_pred_lstm, target_names=label_encoder.classes_, zero_division=0)\n",
        "conf_matrix_lstm = confusion_matrix(y_test_encoded, y_pred_lstm)\n",
        "\n",
        "print(\"\\nClassification Report LSTM:\")\n",
        "print(report_lstm)\n",
        "\n",
        "print(\"\\nConfusion Matrix LSTM:\")\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(conf_matrix_lstm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix - LSTM')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SEL 8: INFERENCE (MENGGUNAKAN MODEL TERBAIK)\n",
        "\n",
        "Bagian ini menunjukkan cara menggunakan model yang telah dilatih (dipilih yang terbaik, biasanya LSTM jika akurasinya tinggi) untuk memprediksi sentimen dari teks baru. -> **Memenuhi Saran ke-6: Melakukan Inference**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pilih model yang akan digunakan untuk inference\n",
        "# Biasanya model dengan performa terbaik di test set, misal LSTM\n",
        "print(\"\\n--- INFERENCE MENGGUNAKAN MODEL LSTM ---\")\n",
        "\n",
        "inference_model = lstm_model\n",
        "inference_tokenizer = keras_tokenizer # Gunakan tokenizer yang SAMA saat training LSTM\n",
        "inference_label_encoder = label_encoder # Gunakan encoder yang SAMA\n",
        "\n",
        "def predict_sentiment(text):\n",
        "    \"\"\"Fungsi untuk memprediksi sentimen teks baru.\"\"\"\n",
        "    # 1. Cleaning teks input\n",
        "    #    Penting: Gunakan fungsi clean_text yang sama persis dengan saat preprocessing!\n",
        "    cleaned_text = clean_text(text)\n",
        "    print(f\"Teks setelah cleaning: '{cleaned_text}'\") # Debugging\n",
        "    if not cleaned_text:\n",
        "        print(\"Teks kosong setelah cleaning, tidak dapat diprediksi.\")\n",
        "        # Beri nilai default atau raise error\n",
        "        # Misalnya, kembalikan Netral atau label mayoritas\n",
        "        default_pred_index = label_mapping.get('Netral', 0) # Index Netral atau 0\n",
        "        default_prob = np.zeros(num_classes)\n",
        "        default_prob[default_pred_index] = 1.0\n",
        "        predicted_sentiment = inference_label_encoder.inverse_transform([default_pred_index])[0]\n",
        "        return predicted_sentiment, default_prob\n",
        "\n",
        "    # 2. Konversi ke sekuens\n",
        "    sequence = inference_tokenizer.texts_to_sequences([cleaned_text])\n",
        "\n",
        "    # 3. Padding sekuens\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
        "\n",
        "    # 4. Prediksi menggunakan model\n",
        "    # Pastikan input shape sesuai dengan model.predict()\n",
        "    # print(f\"Shape input prediksi: {padded_sequence.shape}\") # Debugging\n",
        "    try:\n",
        "        prediction_prob = inference_model.predict(padded_sequence, verbose=0)\n",
        "    except Exception as e:\n",
        "        print(f\"Error saat prediksi: {e}\")\n",
        "        # Handle error, mungkin return default\n",
        "        default_pred_index = label_mapping.get('Netral', 0)\n",
        "        default_prob = np.zeros(num_classes)\n",
        "        default_prob[default_pred_index] = 1.0\n",
        "        predicted_sentiment = inference_label_encoder.inverse_transform([default_pred_index])[0]\n",
        "        return predicted_sentiment, default_prob\n",
        "\n",
        "    predicted_class_index = np.argmax(prediction_prob, axis=1)[0]\n",
        "\n",
        "    # 5. Decode hasil prediksi ke label asli\n",
        "    predicted_sentiment = inference_label_encoder.inverse_transform([predicted_class_index])[0]\n",
        "\n",
        "    return predicted_sentiment, prediction_prob[0] # Kembalikan label dan probabilitasnya\n",
        "\n",
        "# Contoh Penggunaan Inference\n",
        "print(\"\\nContoh Inference:\")\n",
        "contoh_ulasan = [\n",
        "    \"Aplikasinya bagus banget, mudah digunakan dan cepat! Suka sekali!\",\n",
        "    \"Kecewa berat, sering error dan lemot sekali app nya. Buang2 kuota.\",\n",
        "    \"Biasa aja sih, fiturnya standar tidak ada yang spesial. Ya lumayan.\",\n",
        "    \"Update terbaru bikin aplikasi jadi aneh dan susah, tolong diperbaiki secepatnya.\",\n",
        "    \"Lumayan lah buat belanja online kebutuhan sehari hari, pengiriman juga cepat top.\",\n",
        "    \"Gak jelas banget aplikasi ini. Crash terus.\",\n",
        "    \"Fitur search nya kurang akurat\",\n",
        "    \"Terima kasih, sangat membantu!\"\n",
        "]\n",
        "\n",
        "# Tampilkan hasil prediksi untuk setiap contoh\n",
        "# !! PENTING: Output cell ini adalah BUKTI INFERENCE untuk submission !!\n",
        "print(\"=\"*40)\n",
        "for ulasan in contoh_ulasan:\n",
        "    print(f\"Ulasan Mentah: {ulasan}\")\n",
        "    prediksi, probabilitas = predict_sentiment(ulasan)\n",
        "    print(f\"Hasil Prediksi: {prediksi}\")\n",
        "    # Tampilkan probabilitas per kelas (opsional)\n",
        "    prob_dict = {label: f\"{prob:.4f}\" for label, prob in zip(inference_label_encoder.classes_, probabilitas)}\n",
        "    print(f\"Probabilitas  : {prob_dict}\")\n",
        "    print(\"-\"*40)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SEL 9: KESIMPULAN & PEMENUHAN SYARAT\n",
        "\n",
        "Meringkas hasil proyek dan memeriksa pemenuhan kriteria wajib serta saran untuk bintang 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n--- KESIMPULAN PROYEK & PEMENUHAN SYARAT ---\")\n",
        "\n",
        "# Cek Kriteria Utama\n",
        "print(\"\\nKriteria Utama:\")\n",
        "print(f\"1. Scraping data mandiri: {'Ya (Diasumsikan dari file CSV)'}\")\n",
        "print(f\"   - Jumlah data (setelah cleaning): {len(df_cleaned)}\")\n",
        "min_data_ok = len(df_cleaned) >= 3000\n",
        "print(f\"   - Minimal 3000 sampel: {'Ya' if min_data_ok else 'TIDAK'}\")\n",
        "\n",
        "print(f\"2. Ekstraksi Fitur & Pelabelan: Ya (TF-IDF, Sequence Padding; {num_classes} kelas sentimen)\")\n",
        "label_3_kelas_ok = num_classes >= 3\n",
        "print(f\"   - Minimal 3 kelas sentimen: {'Ya' if label_3_kelas_ok else 'TIDAK'}\") # Juga bagian dari saran\n",
        "\n",
        "print(f\"3. Menggunakan Algoritma ML/DL: Ya (SVM, Random Forest, LSTM)\")\n",
        "\n",
        "# Cek akurasi minimal 85% untuk ketiga eksperimen\n",
        "acc_svm_ok = accuracy_svm >= 0.85\n",
        "acc_rf_ok = accuracy_rf >= 0.85\n",
        "acc_lstm_ok = accuracy_lstm_test >= 0.85\n",
        "semua_eksperimen_ok = acc_svm_ok and acc_rf_ok and acc_lstm_ok\n",
        "print(f\"4. Akurasi Testing Set >= 85% (minimal 3 skema): {'Ya' if semua_eksperimen_ok else 'TIDAK'}\")\n",
        "print(f\"   - SVM (80/20): {accuracy_svm:.4f} ({'OK' if acc_svm_ok else 'NOK'})\")\n",
        "print(f\"   - RF (70/30): {accuracy_rf:.4f} ({'OK' if acc_rf_ok else 'NOK'})\")\n",
        "print(f\"   - LSTM (80/20): {accuracy_lstm_test:.4f} ({'OK' if acc_lstm_ok else 'NOK'})\")\n",
        "\n",
        "kriteria_utama_terpenuhi = min_data_ok and label_3_kelas_ok and semua_eksperimen_ok\n",
        "print(f\"\\nStatus Kriteria Utama: {'TERPENUHI SEMUA' if kriteria_utama_terpenuhi else 'BELUM TERPENUHI SEMUA'}\")\n",
        "\n",
        "# Cek Saran untuk Nilai Tinggi\n",
        "print(\"\\nSaran untuk Nilai Tinggi (Bintang 5):\")\n",
        "saran_1_dl = True # Menggunakan LSTM\n",
        "saran_2_acc92 = accuracy_lstm_train > 0.92 and accuracy_lstm_test > 0.92\n",
        "saran_3_kelas3 = num_classes >= 3\n",
        "saran_4_data10k = len(df_cleaned) >= 10000\n",
        "saran_5_exp3 = True # Melakukan 3 eksperimen (SVM, RF, LSTM dg variasi)\n",
        "saran_6_inference = True # Ada sel inference\n",
        "\n",
        "print(f\"1. Menggunakan Algoritma Deep Learning: {'Ya' if saran_1_dl else 'Tidak'}\")\n",
        "print(f\"2. Akurasi Train & Test > 92% (untuk DL): {'Ya' if saran_2_acc92 else 'Belum Tercapai'}\")\n",
        "print(f\"3. Dataset memiliki minimal 3 kelas: {'Ya' if saran_3_kelas3 else 'Tidak'}\")\n",
        "print(f\"4. Jumlah data minimal 10.000 sampel: {'Ya' if saran_4_data10k else f'Belum ({len(df_cleaned)})'}\")\n",
        "print(f\"5. Melakukan 3 percobaan skema pelatihan berbeda: {'Ya' if saran_5_exp3 else 'Tidak'}\")\n",
        "print(f\"6. Melakukan inference dalam notebook: {'Ya' if saran_6_inference else 'Tidak'}\")\n",
        "\n",
        "semua_saran_terpenuhi = saran_1_dl and saran_2_acc92 and saran_3_kelas3 and saran_4_data10k and saran_5_exp3 and saran_6_inference\n",
        "jumlah_saran_terpenuhi = sum([saran_1_dl, saran_2_acc92, saran_3_kelas3, saran_4_data10k, saran_5_exp3, saran_6_inference])\n",
        "\n",
        "print(f\"\\nStatus Saran Nilai Tinggi:\")\n",
        "if semua_saran_terpenuhi:\n",
        "    print(\"✅ SEMUA SARAN TERPENUHI - Potensi Bintang 5 (jika Kriteria Utama juga terpenuhi)\")\n",
        "elif jumlah_saran_terpenuhi >= 3:\n",
        "    print(f\"✅ {jumlah_saran_terpenuhi}/6 SARAN TERPENUHI - Potensi Bintang 4 (jika Kriteria Utama juga terpenuhi)\")\n",
        "else:\n",
        "     print(f\"⚠️ {jumlah_saran_terpenuhi}/6 SARAN TERPENUHI - Potensi Bintang 3 (jika Kriteria Utama juga terpenuhi)\")\n",
        "\n",
        "print(\"\\nCatatan Akhir:\")\n",
        "print(\"- Hasil akurasi sangat bergantung pada kualitas data hasil scraping, proses cleaning, dan tuning hyperparameter.\")\n",
        "print(\"- Mungkin perlu penyesuaian (misalnya jumlah epoch, arsitektur model, parameter TfidfVectorizer) untuk mencapai target akurasi.\")\n",
        "print(\"- Pastikan semua file (notebook .ipynb yang sudah dijalankan, kode scraping .py/.ipynb, dataset .csv, requirements.txt) disertakan dalam satu file ZIP untuk submission.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyN4WbF3j0L1yG7tD9zR+kXp",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myvenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
